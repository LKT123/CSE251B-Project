{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE251B Project Milestone Starter File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You need to describe in your own words what the dataset is about, and use mathematical language and formulate your prediction task on the submitted PDF file for Question 1 Problem A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we are loading the dataset from the local directory. And answer Question 1 Problem B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_npz = np.load('cse-251-b-2025/train.npz')\n",
    "train_data = train_npz['data']\n",
    "test_npz  = np.load('cse-251-b-2025/test_input.npz')\n",
    "test_data  = test_npz['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original X_train shape: (10000, 50, 50, 6)\n",
      "Augmented X_train shape: (89784, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data[..., :50, :]\n",
    "\n",
    "aug_X = []\n",
    "max_swaps_per_scene = 8  #\n",
    "for scene in train_data:\n",
    "    vehicle_indices = []\n",
    "\n",
    "    # Find vehicle agents (object_type == 0)\n",
    "    for i in range(1, 50):  # skip agent 0\n",
    "        agent = scene[i]\n",
    "        if np.all(agent == 0):\n",
    "            continue\n",
    "        if agent[0, 5] == 0:  # 'vehicle'\n",
    "            vehicle_indices.append(i)\n",
    "\n",
    "    # Randomly select up to max_swaps_per_scene\n",
    "    selected = np.random.choice(vehicle_indices, size=min(max_swaps_per_scene, len(vehicle_indices)), replace=False)\n",
    "\n",
    "    for i in selected:\n",
    "        swapped_scene = scene.copy()\n",
    "        swapped_scene[[0, i]] = swapped_scene[[i, 0]]\n",
    "        aug_X.append(swapped_scene[:, :50, :])  # (50, 50, 6)\n",
    "\n",
    "# Stack and combine\n",
    "aug_X = np.stack(aug_X)                          # (N_aug, 50, 50, 6)\n",
    "X_train_aug = np.concatenate([X_train, aug_X], axis=0)\n",
    "\n",
    "print(\"Original X_train shape:\", X_train.shape)\n",
    "print(\"Augmented X_train shape:\", X_train_aug.shape)\n",
    "\n",
    "X_train = X_train_aug\n",
    "Y_train = train_data[:, 0, 50:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(data, title=None, bins=5):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "\n",
    "    x_max = data[..., 0].max()\n",
    "    x_min = data[..., 0].min()\n",
    "    y_max = data[..., 1].max()\n",
    "    y_min = data[..., 1].min()\n",
    "\n",
    "    plt.hist2d(data[:, 0], data[:, 1], bins=bins, cmap='hot')\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.title(title)\n",
    "    plt.colorbar(label='Density')\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy_in = train_data[:, :, :50, :2].reshape(-1, 2)\n",
    "# only find the x, y != 0\n",
    "xy_in_not_0 = xy_in[(xy_in[:, 0] != 0) & (xy_in[:, 1] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_heatmap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplot_heatmap\u001b[49m(xy_in, title=\u001b[33m'\u001b[39m\u001b[33mHeatmap of XY In\u001b[39m\u001b[33m'\u001b[39m, bins=\u001b[32m5\u001b[39m)\n\u001b[32m      2\u001b[39m plot_heatmap(xy_in_not_0, title=\u001b[33m'\u001b[39m\u001b[33mHeatmap of XY In (non-zero)\u001b[39m\u001b[33m'\u001b[39m, bins=\u001b[32m5\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plot_heatmap' is not defined"
     ]
    }
   ],
   "source": [
    "plot_heatmap(xy_in, title='Heatmap of XY In', bins=5)\n",
    "plot_heatmap(xy_in_not_0, title='Heatmap of XY In (non-zero)', bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(xy_in, title='Heatmap of XY In', bins=50)\n",
    "plot_heatmap(xy_in_not_0, title='Heatmap of XY In (non-zero)', bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to play around with dataset for training and testing, make exploratory analysis on the dataset for bonus points(up to 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setting up the Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDatasetTrain(Dataset):\n",
    "    def __init__(self, data, scale=10.0, augment=True):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Training data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        augment: Whether to apply data augmentation (only for training)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene = self.data[idx]\n",
    "        # Getting 50 historical timestamps and 60 future timestamps\n",
    "        hist = scene[:, :50, :].copy()    # (agents=50, time_seq=50, 6)\n",
    "        future = torch.tensor(scene[0, 50:, :2].copy(), dtype=torch.float32)  # (60, 2)\n",
    "        \n",
    "        # Data augmentation(only for training)\n",
    "        if self.augment:\n",
    "            if np.random.rand() < 0.5:\n",
    "                theta = np.random.uniform(-np.pi, np.pi)\n",
    "                R = np.array([[np.cos(theta), -np.sin(theta)],\n",
    "                              [np.sin(theta),  np.cos(theta)]], dtype=np.float32)\n",
    "                # Rotate the historical trajectory and future trajectory\n",
    "                hist[..., :2] = hist[..., :2] @ R\n",
    "                hist[..., 2:4] = hist[..., 2:4] @ R\n",
    "                future = future @ R\n",
    "            if np.random.rand() < 0.5:\n",
    "                hist[..., 0] *= -1\n",
    "                hist[..., 2] *= -1\n",
    "                future[:, 0] *= -1\n",
    "\n",
    "        # Use the last timeframe of the historical trajectory as the origin\n",
    "        origin = hist[0, 49, :2].copy()  # (2,)\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        future = future - origin\n",
    "\n",
    "        # Normalize the historical trajectory and future trajectory\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "        future = future / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            y=future.type(torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "        return data_item\n",
    "    \n",
    "\n",
    "class TrajectoryDatasetTest(Dataset):\n",
    "    def __init__(self, data, scale=10.0):\n",
    "        \"\"\"\n",
    "        data: Shape (N, 50, 110, 6) Testing data\n",
    "        scale: Scale for normalization (suggested to use 10.0 for Argoverse 2 data)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.scale = scale\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Testing data only contains historical trajectory\n",
    "        scene = self.data[idx]  # (50, 50, 6)\n",
    "        hist = scene.copy()\n",
    "        \n",
    "        origin = hist[0, 49, :2].copy()\n",
    "        hist[..., :2] = hist[..., :2] - origin\n",
    "        hist[..., :4] = hist[..., :4] / self.scale\n",
    "\n",
    "        data_item = Data(\n",
    "            x=torch.tensor(hist, dtype=torch.float32),\n",
    "            origin=torch.tensor(origin, dtype=torch.float32).unsqueeze(0),\n",
    "            scale=torch.tensor(self.scale, dtype=torch.float32),\n",
    "        )\n",
    "        return data_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(251)\n",
    "np.random.seed(42)\n",
    "\n",
    "scale = 7.0\n",
    "\n",
    "N = len(train_data)\n",
    "val_size = int(0.1 * N)\n",
    "train_size = N - val_size\n",
    "\n",
    "train_dataset = TrajectoryDatasetTrain(train_data[:train_size], scale=scale, augment=True)\n",
    "val_dataset = TrajectoryDatasetTrain(train_data[train_size:], scale=scale, augment=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: Batch.from_data_list(x))\n",
    "\n",
    "# Set device for training speedup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using Apple Silicon GPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Transformer to Reconstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, T, D)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim=6, d_model=512, nhead=16, num_layers=4, \n",
    "                 output_dim=60 * 2, seq_len=50):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=d_model*4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.predict_head = nn.Sequential(\n",
    "            nn.Linear(d_model * seq_len, d_model*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model*2, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = data.x.view(-1, 50, 50, 6)[:, 0, :, :]  # (B, T, F)\n",
    "        B, T, F = x.shape\n",
    "        \n",
    "        x = self.input_proj(x)  # (B, T, d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        transformer_out = self.transformer(x)  # (B, T, d_model)\n",
    "\n",
    "        flattened = transformer_out.reshape(B, -1)  # (B, T * d_model)\n",
    "        \n",
    "        out = self.predict_head(flattened)  # (B, 120)\n",
    "        return out.view(-1, 60, 2)  # (B, 60, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerModel(d_model=512, nhead=64, num_layers=4).to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[\n",
    "        optim.lr_scheduler.LinearLR(optimizer, start_factor=0.01, total_iters=10),\n",
    "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40, eta_min=1e-6)\n",
    "    ],\n",
    "    milestones=[10]\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.SmoothL1Loss(beta=0.5)\n",
    "\n",
    "early_stopping_patience = 30\n",
    "best_val_loss = float('inf')\n",
    "no_improvement = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|                                                                                | 0/100 [00:00<?, ?epoch/s]C:\\Users\\dysr3\\AppData\\Local\\Temp\\ipykernel_11284\\3713195397.py:30: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  future = future @ R\n",
      "C:\\Users\\dysr3\\AppData\\Local\\Temp\\ipykernel_11284\\3713195397.py:39: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  future = future - origin\n",
      "Epoch:   1%|▋                                                                       | 1/100 [00:09<15:06,  9.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | LR 0.000033 | Train Loss 0.6456 | Val Loss 0.4200 | Val MAE 4.0030 | Val MSE 51.1283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   2%|█▍                                                                      | 2/100 [00:18<15:02,  9.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | LR 0.000062 | Train Loss 0.3597 | Val Loss 0.2927 | Val MAE 3.1312 | Val MSE 29.2511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   3%|██▏                                                                     | 3/100 [00:27<14:50,  9.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 | LR 0.000092 | Train Loss 0.3198 | Val Loss 0.2575 | Val MAE 2.9388 | Val MSE 23.5629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   4%|██▉                                                                     | 4/100 [00:36<14:31,  9.08s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 | LR 0.000122 | Train Loss 0.2684 | Val Loss 0.2854 | Val MAE 3.2189 | Val MSE 24.8803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|███▌                                                                    | 5/100 [00:45<14:21,  9.07s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 | LR 0.000151 | Train Loss 0.2364 | Val Loss 0.2426 | Val MAE 2.9256 | Val MSE 19.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   6%|████▎                                                                   | 6/100 [00:54<14:06,  9.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 | LR 0.000181 | Train Loss 0.2243 | Val Loss 0.2116 | Val MAE 2.5824 | Val MSE 17.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   7%|█████                                                                   | 7/100 [01:03<13:51,  8.95s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 | LR 0.000211 | Train Loss 0.2220 | Val Loss 0.2007 | Val MAE 2.4291 | Val MSE 17.2817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   8%|█████▊                                                                  | 8/100 [01:12<13:39,  8.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 | LR 0.000241 | Train Loss 0.2194 | Val Loss 0.1751 | Val MAE 2.1897 | Val MSE 14.8366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   9%|██████▍                                                                 | 9/100 [01:21<13:36,  8.97s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 | LR 0.000270 | Train Loss 0.2194 | Val Loss 0.1971 | Val MAE 2.3819 | Val MSE 17.4312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|███████                                                                | 10/100 [01:30<13:37,  9.08s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 | LR 0.000300 | Train Loss 0.2236 | Val Loss 0.2337 | Val MAE 2.7662 | Val MSE 19.8968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  11%|███████▊                                                               | 11/100 [01:39<13:37,  9.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | LR 0.000300 | Train Loss 0.2160 | Val Loss 0.2021 | Val MAE 2.4606 | Val MSE 16.9119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  12%|████████▌                                                              | 12/100 [01:49<13:33,  9.24s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 | LR 0.000298 | Train Loss 0.2034 | Val Loss 0.2279 | Val MAE 2.7036 | Val MSE 19.4298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  13%|█████████▏                                                             | 13/100 [01:58<13:30,  9.32s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012 | LR 0.000296 | Train Loss 0.2036 | Val Loss 0.1657 | Val MAE 2.0858 | Val MSE 13.9908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  14%|█████████▉                                                             | 14/100 [02:08<13:26,  9.38s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 | LR 0.000293 | Train Loss 0.1834 | Val Loss 0.1532 | Val MAE 1.9868 | Val MSE 12.6797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|██████████▋                                                            | 15/100 [02:17<13:17,  9.38s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014 | LR 0.000289 | Train Loss 0.1767 | Val Loss 0.1761 | Val MAE 2.1913 | Val MSE 14.2747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  16%|███████████▎                                                           | 16/100 [02:26<13:06,  9.36s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 | LR 0.000284 | Train Loss 0.1709 | Val Loss 0.1913 | Val MAE 2.4211 | Val MSE 15.5339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  17%|████████████                                                           | 17/100 [02:36<12:48,  9.26s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 | LR 0.000278 | Train Loss 0.1665 | Val Loss 0.1683 | Val MAE 2.1067 | Val MSE 13.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  18%|████████████▊                                                          | 18/100 [02:45<12:35,  9.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017 | LR 0.000271 | Train Loss 0.1543 | Val Loss 0.1679 | Val MAE 2.0383 | Val MSE 14.5839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  19%|█████████████▍                                                         | 19/100 [02:54<12:19,  9.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018 | LR 0.000264 | Train Loss 0.1543 | Val Loss 0.1419 | Val MAE 1.8256 | Val MSE 11.9551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██████████████▏                                                        | 20/100 [03:02<12:02,  9.04s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 | LR 0.000256 | Train Loss 0.1503 | Val Loss 0.1422 | Val MAE 1.8729 | Val MSE 11.5029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  21%|██████████████▉                                                        | 21/100 [03:11<11:50,  9.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | LR 0.000248 | Train Loss 0.1455 | Val Loss 0.1263 | Val MAE 1.6150 | Val MSE 10.4687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  22%|███████████████▌                                                       | 22/100 [03:20<11:36,  8.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021 | LR 0.000238 | Train Loss 0.1427 | Val Loss 0.1296 | Val MAE 1.6230 | Val MSE 10.7492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  23%|████████████████▎                                                      | 23/100 [03:29<11:24,  8.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 | LR 0.000229 | Train Loss 0.1412 | Val Loss 0.1452 | Val MAE 1.9348 | Val MSE 11.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  24%|█████████████████                                                      | 24/100 [03:38<11:12,  8.85s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 023 | LR 0.000218 | Train Loss 0.1398 | Val Loss 0.1270 | Val MAE 1.6626 | Val MSE 10.1630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|█████████████████▊                                                     | 25/100 [03:47<11:06,  8.88s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 | LR 0.000208 | Train Loss 0.1376 | Val Loss 0.1204 | Val MAE 1.5978 | Val MSE 9.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  26%|██████████████████▍                                                    | 26/100 [03:55<10:55,  8.86s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 | LR 0.000197 | Train Loss 0.1339 | Val Loss 0.1315 | Val MAE 1.7308 | Val MSE 10.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  27%|███████████████████▏                                                   | 27/100 [04:04<10:49,  8.89s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 | LR 0.000185 | Train Loss 0.1330 | Val Loss 0.1178 | Val MAE 1.6024 | Val MSE 9.4706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  28%|███████████████████▉                                                   | 28/100 [04:13<10:40,  8.90s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 027 | LR 0.000174 | Train Loss 0.1297 | Val Loss 0.1210 | Val MAE 1.6502 | Val MSE 9.8423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  29%|████████████████████▌                                                  | 29/100 [04:23<10:42,  9.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 | LR 0.000162 | Train Loss 0.1287 | Val Loss 0.1132 | Val MAE 1.5335 | Val MSE 9.2464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|█████████████████████▎                                                 | 30/100 [04:32<10:38,  9.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 029 | LR 0.000150 | Train Loss 0.1252 | Val Loss 0.1141 | Val MAE 1.5060 | Val MSE 9.1675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  31%|██████████████████████                                                 | 31/100 [04:41<10:31,  9.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 | LR 0.000139 | Train Loss 0.1253 | Val Loss 0.1173 | Val MAE 1.5587 | Val MSE 9.6117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  32%|██████████████████████▋                                                | 32/100 [04:51<10:27,  9.22s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 031 | LR 0.000127 | Train Loss 0.1233 | Val Loss 0.1113 | Val MAE 1.4886 | Val MSE 8.8747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  33%|███████████████████████▍                                               | 33/100 [05:00<10:18,  9.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 | LR 0.000116 | Train Loss 0.1219 | Val Loss 0.1123 | Val MAE 1.5398 | Val MSE 9.1236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  34%|████████████████████████▏                                              | 34/100 [05:09<10:09,  9.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 | LR 0.000104 | Train Loss 0.1203 | Val Loss 0.1164 | Val MAE 1.5084 | Val MSE 9.6655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  35%|████████████████████████▊                                              | 35/100 [05:18<09:56,  9.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 | LR 0.000093 | Train Loss 0.1177 | Val Loss 0.1183 | Val MAE 1.5807 | Val MSE 9.6715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  36%|█████████████████████████▌                                             | 36/100 [05:27<09:38,  9.04s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035 | LR 0.000083 | Train Loss 0.1162 | Val Loss 0.1261 | Val MAE 1.6214 | Val MSE 10.3177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  37%|██████████████████████████▎                                            | 37/100 [05:36<09:26,  9.00s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036 | LR 0.000072 | Train Loss 0.1141 | Val Loss 0.1110 | Val MAE 1.4823 | Val MSE 9.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  38%|██████████████████████████▉                                            | 38/100 [05:45<09:20,  9.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 037 | LR 0.000063 | Train Loss 0.1114 | Val Loss 0.1043 | Val MAE 1.3330 | Val MSE 8.9001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  39%|███████████████████████████▋                                           | 39/100 [05:54<09:07,  8.98s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038 | LR 0.000053 | Train Loss 0.1098 | Val Loss 0.1074 | Val MAE 1.4603 | Val MSE 8.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████████████████████████████▍                                          | 40/100 [06:03<08:56,  8.94s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039 | LR 0.000045 | Train Loss 0.1093 | Val Loss 0.1084 | Val MAE 1.4016 | Val MSE 8.8755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  41%|█████████████████████████████                                          | 41/100 [06:11<08:45,  8.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040 | LR 0.000037 | Train Loss 0.1071 | Val Loss 0.1037 | Val MAE 1.3439 | Val MSE 8.6820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  42%|█████████████████████████████▊                                         | 42/100 [06:20<08:37,  8.92s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 041 | LR 0.000030 | Train Loss 0.1061 | Val Loss 0.1135 | Val MAE 1.4596 | Val MSE 9.4986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  43%|██████████████████████████████▌                                        | 43/100 [06:30<08:36,  9.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042 | LR 0.000023 | Train Loss 0.1044 | Val Loss 0.1026 | Val MAE 1.3263 | Val MSE 8.3740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  44%|███████████████████████████████▏                                       | 44/100 [06:39<08:31,  9.14s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043 | LR 0.000017 | Train Loss 0.1037 | Val Loss 0.1023 | Val MAE 1.3174 | Val MSE 8.5373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|███████████████████████████████▉                                       | 45/100 [06:48<08:26,  9.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044 | LR 0.000012 | Train Loss 0.1037 | Val Loss 0.1018 | Val MAE 1.3335 | Val MSE 8.4421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  46%|████████████████████████████████▋                                      | 46/100 [06:58<08:20,  9.27s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 045 | LR 0.000008 | Train Loss 0.1021 | Val Loss 0.1007 | Val MAE 1.2967 | Val MSE 8.3595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  47%|█████████████████████████████████▎                                     | 47/100 [07:07<08:12,  9.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 046 | LR 0.000005 | Train Loss 0.1010 | Val Loss 0.1002 | Val MAE 1.2816 | Val MSE 8.4338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  48%|██████████████████████████████████                                     | 48/100 [07:16<08:02,  9.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 | LR 0.000003 | Train Loss 0.1010 | Val Loss 0.1019 | Val MAE 1.3030 | Val MSE 8.4977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  49%|██████████████████████████████████▊                                    | 49/100 [07:26<07:53,  9.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048 | LR 0.000001 | Train Loss 0.1012 | Val Loss 0.1005 | Val MAE 1.2841 | Val MSE 8.4091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|███████████████████████████████████▌                                   | 50/100 [07:35<07:44,  9.30s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049 | LR 0.000001 | Train Loss 0.1007 | Val Loss 0.1001 | Val MAE 1.2745 | Val MSE 8.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  51%|████████████████████████████████████▏                                  | 51/100 [07:44<07:30,  9.20s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050 | LR 0.000001 | Train Loss 0.0999 | Val Loss 0.1003 | Val MAE 1.2807 | Val MSE 8.3952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  52%|████████████████████████████████████▉                                  | 52/100 [07:53<07:22,  9.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 051 | LR 0.000003 | Train Loss 0.1000 | Val Loss 0.0995 | Val MAE 1.2706 | Val MSE 8.3416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  53%|█████████████████████████████████████▋                                 | 53/100 [08:02<07:10,  9.16s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 052 | LR 0.000005 | Train Loss 0.1010 | Val Loss 0.0988 | Val MAE 1.2574 | Val MSE 8.2853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  54%|██████████████████████████████████████▎                                | 54/100 [08:11<06:56,  9.06s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 053 | LR 0.000008 | Train Loss 0.1006 | Val Loss 0.1009 | Val MAE 1.2897 | Val MSE 8.4123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|███████████████████████████████████████                                | 55/100 [08:20<06:43,  8.96s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 054 | LR 0.000012 | Train Loss 0.1009 | Val Loss 0.0997 | Val MAE 1.2728 | Val MSE 8.3152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  56%|███████████████████████████████████████▊                               | 56/100 [08:29<06:32,  8.91s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 055 | LR 0.000017 | Train Loss 0.1011 | Val Loss 0.0991 | Val MAE 1.2846 | Val MSE 8.2249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  57%|████████████████████████████████████████▍                              | 57/100 [08:38<06:27,  9.02s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 056 | LR 0.000023 | Train Loss 0.1019 | Val Loss 0.1034 | Val MAE 1.3442 | Val MSE 8.7183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  58%|█████████████████████████████████████████▏                             | 58/100 [08:47<06:21,  9.09s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 057 | LR 0.000030 | Train Loss 0.1037 | Val Loss 0.1013 | Val MAE 1.3060 | Val MSE 8.4388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  59%|█████████████████████████████████████████▉                             | 59/100 [08:56<06:14,  9.14s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 058 | LR 0.000037 | Train Loss 0.1026 | Val Loss 0.1027 | Val MAE 1.3246 | Val MSE 8.6262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████████████████████████████████████████▌                            | 60/100 [09:06<06:07,  9.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 059 | LR 0.000045 | Train Loss 0.1038 | Val Loss 0.1019 | Val MAE 1.3199 | Val MSE 8.5942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  61%|███████████████████████████████████████████▎                           | 61/100 [09:15<05:58,  9.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 060 | LR 0.000053 | Train Loss 0.1041 | Val Loss 0.1068 | Val MAE 1.3874 | Val MSE 8.7825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  62%|████████████████████████████████████████████                           | 62/100 [09:24<05:50,  9.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 061 | LR 0.000063 | Train Loss 0.1061 | Val Loss 0.1047 | Val MAE 1.3839 | Val MSE 8.6417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  63%|████████████████████████████████████████████▋                          | 63/100 [09:33<05:41,  9.24s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 062 | LR 0.000072 | Train Loss 0.1069 | Val Loss 0.1071 | Val MAE 1.4268 | Val MSE 8.8099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  64%|█████████████████████████████████████████████▍                         | 64/100 [09:43<05:32,  9.24s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 063 | LR 0.000083 | Train Loss 0.1089 | Val Loss 0.1027 | Val MAE 1.3461 | Val MSE 8.5386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████████████████████████████████████████████▏                        | 65/100 [09:52<05:23,  9.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 064 | LR 0.000093 | Train Loss 0.1088 | Val Loss 0.1078 | Val MAE 1.4203 | Val MSE 8.9782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  66%|██████████████████████████████████████████████▊                        | 66/100 [10:01<05:14,  9.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 065 | LR 0.000104 | Train Loss 0.1110 | Val Loss 0.1178 | Val MAE 1.5389 | Val MSE 9.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  67%|███████████████████████████████████████████████▌                       | 67/100 [10:10<05:03,  9.19s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 066 | LR 0.000116 | Train Loss 0.1112 | Val Loss 0.1090 | Val MAE 1.5015 | Val MSE 8.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  68%|████████████████████████████████████████████████▎                      | 68/100 [10:19<04:48,  9.01s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 067 | LR 0.000127 | Train Loss 0.1141 | Val Loss 0.1110 | Val MAE 1.4911 | Val MSE 9.1882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  69%|████████████████████████████████████████████████▉                      | 69/100 [10:28<04:37,  8.95s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 068 | LR 0.000139 | Train Loss 0.1138 | Val Loss 0.1120 | Val MAE 1.4607 | Val MSE 9.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|█████████████████████████████████████████████████▋                     | 70/100 [10:37<04:31,  9.05s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 069 | LR 0.000151 | Train Loss 0.1160 | Val Loss 0.1199 | Val MAE 1.5829 | Val MSE 9.7894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  71%|██████████████████████████████████████████████████▍                    | 71/100 [10:46<04:24,  9.13s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 070 | LR 0.000162 | Train Loss 0.1172 | Val Loss 0.1129 | Val MAE 1.4771 | Val MSE 9.2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  72%|███████████████████████████████████████████████████                    | 72/100 [10:56<04:16,  9.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 071 | LR 0.000174 | Train Loss 0.1189 | Val Loss 0.1072 | Val MAE 1.4619 | Val MSE 8.5840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  73%|███████████████████████████████████████████████████▊                   | 73/100 [11:05<04:07,  9.18s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 072 | LR 0.000185 | Train Loss 0.1193 | Val Loss 0.1330 | Val MAE 1.7290 | Val MSE 10.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  74%|████████████████████████████████████████████████████▌                  | 74/100 [11:14<03:57,  9.12s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 073 | LR 0.000197 | Train Loss 0.1204 | Val Loss 0.1188 | Val MAE 1.5947 | Val MSE 9.8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|█████████████████████████████████████████████████████▎                 | 75/100 [11:23<03:49,  9.17s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 074 | LR 0.000208 | Train Loss 0.1217 | Val Loss 0.1103 | Val MAE 1.4708 | Val MSE 9.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  76%|█████████████████████████████████████████████████████▉                 | 76/100 [11:32<03:41,  9.21s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 075 | LR 0.000218 | Train Loss 0.1224 | Val Loss 0.1158 | Val MAE 1.5611 | Val MSE 9.4219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  77%|██████████████████████████████████████████████████████▋                | 77/100 [11:42<03:32,  9.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 076 | LR 0.000229 | Train Loss 0.1216 | Val Loss 0.1108 | Val MAE 1.4945 | Val MSE 8.8026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  78%|███████████████████████████████████████████████████████▍               | 78/100 [11:51<03:23,  9.23s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 077 | LR 0.000238 | Train Loss 0.1237 | Val Loss 0.1175 | Val MAE 1.5182 | Val MSE 9.7302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  79%|████████████████████████████████████████████████████████               | 79/100 [12:00<03:14,  9.25s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 078 | LR 0.000248 | Train Loss 0.1263 | Val Loss 0.1274 | Val MAE 1.6731 | Val MSE 10.2905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████████████████████████████████████████████████████▊              | 80/100 [12:10<03:05,  9.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 079 | LR 0.000256 | Train Loss 0.1259 | Val Loss 0.1101 | Val MAE 1.4739 | Val MSE 8.7418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  81%|█████████████████████████████████████████████████████████▌             | 81/100 [12:19<02:56,  9.29s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 080 | LR 0.000264 | Train Loss 0.1261 | Val Loss 0.1241 | Val MAE 1.6682 | Val MSE 9.6419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  82%|██████████████████████████████████████████████████████████▏            | 82/100 [12:28<02:47,  9.28s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 081 | LR 0.000271 | Train Loss 0.1268 | Val Loss 0.1402 | Val MAE 1.8702 | Val MSE 11.2679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  82%|██████████████████████████████████████████████████████████▏            | 82/100 [12:37<02:46,  9.24s/epoch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 082 | LR 0.000278 | Train Loss 0.1256 | Val Loss 0.1277 | Val MAE 1.7105 | Val MSE 10.4434\n",
      "Early stopping triggered!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm.tqdm(range(100), desc=\"Epoch\", unit=\"epoch\"):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        pred = model(batch)\n",
    "        y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "        loss = criterion(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_mae = 0\n",
    "    val_mse = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            batch = batch.to(device)\n",
    "            pred = model(batch)\n",
    "            y = batch.y.view(batch.num_graphs, 60, 2)\n",
    "            val_loss += criterion(pred, y).item()\n",
    "\n",
    "            pred_denorm = pred * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "            y_denorm = y * batch.scale.view(-1, 1, 1) + batch.origin.unsqueeze(1)\n",
    "            \n",
    "            val_mae += nn.L1Loss()(pred_denorm, y_denorm).item()\n",
    "            val_mse += nn.MSELoss()(pred_denorm, y_denorm).item()\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "    val_loss /= len(val_dataloader)\n",
    "    val_mae /= len(val_dataloader)\n",
    "    val_mse /= len(val_dataloader)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    tqdm.tqdm.write(f\"Epoch {epoch:03d} | LR {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "                   f\"Train Loss {train_loss:.4f} | Val Loss {val_loss:.4f} | \"\n",
    "                   f\"Val MAE {val_mae:.4f} | Val MSE {val_mse:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss - 1e-4:\n",
    "        best_val_loss = val_loss\n",
    "        no_improvement = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        no_improvement += 1\n",
    "        if no_improvement >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TrajectoryDatasetTest(test_data, scale=scale)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n",
    "                         collate_fn=lambda xs: Batch.from_data_list(xs))\n",
    "\n",
    "best_model = torch.load(\"best_model.pt\")\n",
    "model = TransformerModel(d_model=512, nhead=64, num_layers=4).to(device)\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "pred_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = batch.to(device)\n",
    "        pred_norm = model(batch)\n",
    "        \n",
    "        pred = pred_norm * batch.scale.view(-1,1,1) + batch.origin.unsqueeze(1)\n",
    "        pred_list.append(pred.cpu().numpy())\n",
    "\n",
    "pred_list = np.concatenate(pred_list, axis=0)\n",
    "pred_output = pred_list.reshape(-1, 2)\n",
    "output_df = pd.DataFrame(pred_output, columns=['x', 'y'])\n",
    "output_df.index.name = 'index'\n",
    "output_df.to_csv('submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
